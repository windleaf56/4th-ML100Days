{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作業\n",
    "* 選擇分類資料集(右下)-螺旋雙臂 - 交錯六群，限定特徵只能使用前兩個，隱藏層 2 層滿 (共 8 * 2 =16 個神經元)，  \n",
    "  遞迴次數只允許跑到500次，但可以自由調整 批次大小、學習速率、啟動函數、正規化選項與參數\n",
    "* 在上述限制下，挑戰看看測試誤差 (Test Loss) 最低能到多少? 請回答你的上述幾項參數與 Test Loss 數"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 螺旋雙臂\n",
    "Batch size, Learning rate, Activation, Regularization, Regularization rate, Test Loss, Train loss  \n",
    "10, 0.1, Tanh, None, 0, 0.089, 0.060  \n",
    "15, 0.1, Tanh, None, 0, 0.066, 0.005  \n",
    "17, 0.1, Tanh, None, 0, 0.125, 0.072  \n",
    "5, 0.3, Tanh, None, 0, 0.352, 0.345  \n",
    "20, 0.03, Tanh, None, 0, 0.390, 0.322  \n",
    "30, 0.03, Tanh, None, 0, 0.473, 0.415  \n",
    "10, 0.03, Tanh, None, 0, 0.417, 0.356  \n",
    "16, 0.1, Tanh, None, 0, 0.297, 0.204  \n",
    "15, 0.1, Tanh, None, 0, 0.184, 0.092  \n",
    "30, 0.1, Tanh, None, 0, 0.264, 0.194  \n",
    "20, 0.1, Tanh, None, 0, 0.195, 0.105  \n",
    "20, 0.1, Tanh, L2, 0.003, 0.488, 0.424  \n",
    "15, 0.1, Tanh, L2, 0.003, 0.280, 0.230  \n",
    "15, 0.1, Tanh, L2, 0.001, 0.314, 0.244  \n",
    "15, 0.1, Tanh, L2, 0.01, 0.492, 0.462  \n",
    "15, 0.1, Tanh, L1, 0.01, 0.492, 0.462  \n",
    "15, 0.1, Tanh, L1, 0.003, 0.509, 0.454  \n",
    "15, 0.1, Tanh, L1, 0.03, 0.499, 0.502  \n",
    "15, 0.1, Tanh, L1, 0.01, 0.499, 0.502  \n",
    "\n",
    "15, 0.1, Sigmoid, L1, 0.03, 0.499, 0.502  \n",
    "15, 0.1, Sigmoid, None, 0, 0.491, 0.463  \n",
    "10, 0.03, Sigmoid, None, 0, 0.484, 0.466  \n",
    "30, 0.03, Sigmoid, None, 0, 0.485, 0.466  \n",
    "10, 0.3, Sigmoid, None, 0, 0.492, 0.429  \n",
    "15, 0.1, Sigmoid, None, 0, 0.491, 0.463  \n",
    "\n",
    "10, 0.01, ReLU, None, 0, 0.469, 0.394  \n",
    "15, 0.01, ReLU, None, 0, 0.426, 0.370  \n",
    "20, 0.01, ReLU, None, 0, 0.454, 0.401  \n",
    "15, 0.03, ReLU, None, 0, 0.247, 0.182  \n",
    "12, 0.03, ReLU, None, 0, 0.303, 0.174  \n",
    "20, 0.03, ReLU, None, 0, 0.170, 0.096  \n",
    "25, 0.03, ReLU, None, 0, 0.465, 0.388  \n",
    "22, 0.03, ReLU, None, 0, 0.382, 0.327  \n",
    "20, 0.03, ReLU, None, 0, 0.356, 0.323  \n",
    "20, 0.03, ReLU, None, 0, 0.442, 0.351  \n",
    "15, 0.03, ReLU, None, 0, 0.227, 0.165  \n",
    "15, 0.03, ReLU, L2, 0.001, 0.339, 0.298  \n",
    "\n",
    "10, 0.03, Linear, None, 0, 0.480, 0.465  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear跟Sigmod都做不到太好的效果(Loss只能略小於0.5)。  \n",
    "ReLU能到0.170(20, 0.03, ReLU, None, 0, 0.170, 0.096)。  \n",
    "Tanh則是能0.066(15, 0.1, Tanh, None, 0, 0.066, 0.005)。  \n",
    "這兩個啟動函數，以同樣參數再做一次會不一定會得到相同的Loss，我自己後來再做都是看到更高的Loss。  \n",
    "正規化不論是L2或L1都沒辦法對結果產生好影響。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 交錯六群\n",
    "Batch size, Learning rate, Activation, Regularization, Regularization rate, Test Loss, Train loss  \n",
    "Batch size, Learning rate, Activation, Regularization, Regularization rate, Test Loss, Train loss  \n",
    "15, 0.1, Tanh, None, 0, 0.011, 0.010  \n",
    "20, 0.1, Tanh, None, 0, 0.011, 0.011  \n",
    "20, 0.03, Tanh, None, 0, 0.009, 0.011  \n",
    "16, 0.03, Tanh, None, 0, 0.010, 0.011  \n",
    "30, 0.03, Tanh, None, 0, 0.011, 0.012  \n",
    "10, 0.1, Tanh, None, 0, 0.009, 0.008  \n",
    "10, 0.3, Tanh, None, 0, 0.003, 0.003  \n",
    "10, 1, Tanh, None, 0, 0.009, 0.008  \n",
    "10, 1, Tanh, L2, 0.001, 0.012, 0.012  \n",
    "10, 1, Tanh, None, 0, 0.009, 0.008  \n",
    "15, 0.3, Tanh, None, 0, 0.005, 0.004  \n",
    "5, 0.3, Tanh, None, 0, 0.008, 0.007  \n",
    "\n",
    "10, 0.03, ReLU, None, 0, 0.010, 0.009  \n",
    "15, 0.03, ReLU, None, 0, 0.018, 0.019  \n",
    "10, 0.1, ReLU, None, 0, 0.007, 0.006  \n",
    "10, 0.3, ReLU, None, 0, 0.008, 0.006  \n",
    "15, 0.3, ReLU, None, 0, 0.006, 0.005  \n",
    "\n",
    "15, 0.3, Sigmoid, None, 0, 0.015, 0.016  \n",
    "10, 0.3, Sigmoid, None, 0, 0.014, 0.016  \n",
    "10, 1, Sigmoid, None, 0, 0.010, 0.009  \n",
    "\n",
    "Linear很奇怪，LR大於等於1，Loss直接是Nan。  \n",
    "LR小於1，早早就收斂到0.035。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基本上所有函數都很快就收斂，但要再降低Loss就很難。  \n",
    "(10, 0.3, Tanh, None, 0, 0.003, 0.003)，使用Tanh的時候Loss能到0.003。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
